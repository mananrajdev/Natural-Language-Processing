{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19dae114",
   "metadata": {},
   "source": [
    "# MANAN RAJDEV - CSCI 544 - HW4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6056b54d",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0eea2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import numpy as np\n",
    "import warnings\n",
    "import sys\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7f4063c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3aa53bc",
   "metadata": {},
   "source": [
    "## Task 1 - Simple Bidirectional LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e1d07a",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "758c6e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dictionary for mapping word to integer and label to integer for creating their embeddings\n",
    "df_train=pd.read_csv(\"data/train\", sep=\"\\s\", names=[\"idx\",\"word\",\"tag\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72bf4a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count=df_train['word'].value_counts()\n",
    "word_count=word_count[word_count>1]\n",
    "word_set=set(word_count.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1842260",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dictionary for mapping word to integer and label to integer for creating their embeddings\n",
    "#convert training into list of lists of sentences of integers (mapped through the dictionary)\n",
    "train_sentences = []        \n",
    "train_labels = []\n",
    "temp_sentence=[]\n",
    "temp_label=[]\n",
    "word2idx={}\n",
    "label2idx={}\n",
    "\n",
    "word_idx=1\n",
    "label_idx=1\n",
    "with open(\"data/train\") as f:\n",
    "    for sentence in f.read().splitlines():\n",
    "        if sentence==\"\":\n",
    "            if temp_sentence.count(\"UNK\")!=len(temp_sentence):\n",
    "                train_sentences.append(temp_sentence)\n",
    "                train_labels.append(temp_label)\n",
    "            temp_sentence=[]\n",
    "            temp_label=[]\n",
    "            continue\n",
    "        _,word,label=sentence.split()\n",
    "        \n",
    "        word=word if word in word_set else \"UNK\"\n",
    "        if word not in word2idx:\n",
    "            word2idx[word]=word_idx\n",
    "            temp_sentence.append(word_idx)\n",
    "            word_idx+=1\n",
    "        else:\n",
    "            temp_sentence.append(word2idx[word])\n",
    "\n",
    "        if label not in label2idx:\n",
    "            label2idx[label]=label_idx\n",
    "            temp_label.append(label_idx)\n",
    "            label_idx+=1\n",
    "        else:\n",
    "            temp_label.append(label2idx[label])\n",
    "    \n",
    "    train_sentences.append(temp_sentence)\n",
    "    train_labels.append(temp_label)   \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "068ec6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find max length for padding\n",
    "# max_train_sentences = max([len(s) for s in train_sentences])\n",
    "# max_train_sentences\n",
    "list_len=[len(s) for s in train_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a1d96a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Length of sentences will be 30\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Z1A+gAAAACXBIWXMAAAsTAAALEwEAmpwYAAAaDklEQVR4nO3de7AlZX3u8e8DIyIXGS4TAjPooKAGk3jJiCCcFAEjqMQhiSI5Jo4eIlVHouIdcjwHb0SsY+LlJGIRQEEtEBEjakqDCDExAR0kxgxgMeE2g1xGh6sYdfB3/uh3y2Kz9/Samb1m376fqlW7+3378vbqvfez+u1e3akqJEnamG2muwGSpJnPsJAk9TIsJEm9DAtJUi/DQpLUy7CQJPUyLOa5JKuSHDbd7ZhOSX4/yZokDyR51nS3ZyJJDkuydoqWlSQfT3J3km9NxTI19xkWc1iSm5M8f1zZq5L889h4VT29qq7oWc7SJJVkwYiaOt0+APxZVe1UVdeMr2zbvt/WbNCI13ko8LvAkqo6cIJ1b5fkL5OsbQF6c5IPTcWKJ/qd1OwwV//4NYskWVBVG6axCU8EVk3j+re2JwI3V9WPJ6k/BVgGHAjc3qb/7a3UNs1QHlnMc4Of9JIcmGRlkvuS3Jnkr9pk32g/72mfNA9Osk2SdyS5JcldSc5LssvAcl/Z6n6U5H+PW887k1yU5FNJ7gNe1db9r0nuSXJ7kr9Ost3A8irJa5PckOT+JO9J8uQk/9Lae+Hg9OO2ccK2JnlskgeAbYHvJvnPTXzvHpvkA0lube/Xx5I8rtUd1j6Zv7mt8/Ykrx6Yd/ckX2xt/3aS944d8SUZe7+/297vlw/MN+HyJmjb3kkuSbI+yeokr2nlxwNnAQe3Zb9rgtmfA3y+qn5QnZur6rxxy/5cknVJbkry+oG6d7Z9cV7bT6uSLGt1nwSeAHyxrfttrfygth/vSfLdDHSLJrmi7etvtuX9Q5I9BuoPHZh3TZJXDbFv9kjypTbP+iT/lMT/hX2qytccfQE3A88fV/Yq4J8nmgb4V+BP2vBOwEFteClQwIKB+f4HsBp4Upv2YuCTre4A4AG67o7t6Lp5fj6wnne28WPoPrA8Dvgt4CC6o92lwHXASQPrK+ALwOOBpwM/BS5r698FuBZYMcn7MGlbB5a930bexwnrgQ8ClwC7ATsDXwTe1+oOAzYA7wYeA7wIeBDYtdVf0F47tPdrzbj98oh19i1vgrZ9A/gosD3wTGAdcPhEvwMTzPsO4FbgtcBvABmo2wa4Gvg/bd8+CbgROHJg3/5Xa9+2wPuAKyf7nQQWAz9q029D1z32I2BRq78C+E/gKe335Arg9Fb3ROB+4I/ae7I78Mwh9s37gI+1eR4D/LfBbfQ1ye/FdDfA1wh3bveH+QBwz8DrQSYPi28A7wL2GLecpTw6LC4DXjsw/lS6AFjQ/pGcP1C3A/AzHhkW3+hp+0l0n27Hxgs4ZGD8auDtA+N/CXxokmVN2taBZW9SWAABfgw8eaDsYOCmNnwY8JNx79lddIG4bVv/Uwfq3kt/WEy4vAnauw/wELDzQNn7gE+04Vex8bDYFjgR+CZdKP+AFsTAc4Fbx01/CvDxgX37tYG6A4CfTPT71sbfzkBwt7KvDqzvCuAdA3WvBb4ysN7PT9D+vn3zbroPHpPuc1+PfnnoNfcdU1ULx150f2yTOZ7uE9z1rWvk6I1Muzdwy8D4LXRBsWerWzNWUVUP0n1aHLRmcCTJU1rXwB2ta+ovgD3GzXPnwPBPJhjfaTPaurkW0YXg1a074x7gK618zI/qkediHmxtXNTWP/gePOL9mMRkyxtvb2B9Vd0/UHYL3af4XlX1UFX9TVUdAiwETgPOSfJrdJ/m9x7b5rbdf84j38s7xrVx+0x+ccQTgZeNW96hwF4bWd7YNu9Dd9QxXt+++b90R5r/kOTGJCdP0jYNMCz0S1V1Q1X9EfArwPuBi5LsSPcpd7wf0P2hj3kCXTfJnXQnRZeMVbS+4t3Hr27c+BnA9cD+VfV4un9A2fytGbqtm+uHdAH19IEw3qWqJgusQeva+pcMlO2zBW0Z7wfAbkl2Hih7AnDbpi6oqn5SVX8D3M3D3WU3DX4Aqaqdq+pFwy5y3PgauiOLweXtWFWnD7GsNcCTJyjf6L6pqvur6s1V9STgJcCbkhwxZPvnLcNCv5Tkj5Msqqpf0HVZAfyC7p/bL+j6p8ecD7wxyb5JdqI7EvhM++R7EfB7SZ7XTjq/k/5//DsD9wEPJHka8D+naLP62jqs7ZJsP/ai256/BT6Y5FcAkixOcmTfgqrqIbrzJu9MskPb3leOm+xOHvl+D62q1gD/Aryvtfc36Y4aPzXM/ElOaifoH5dkQZIVdPvnGuBbwP1J3t7qt03y60meM2Tzxm/Xp+h+V45sy9q+rXvJJPMP+jTw/CTHtnbunuSZ7fd30n2T5Ogk+yUJcC9dl90vhmz/vGVYaNBRwKp0Vwh9GDiufbJ8kK4r4pvtsP4g4Bzgk3TnOW6iO6n5OoCqWtWGL6A7yniArn/9pxtZ91uA/053wvJvgc9M4XZN2tZNsIru0+rY69V0/e2rgStb19nX6M6HDOPP6E7M39Hadj6PfH/eCZzb3u9jN7Gt0J30XUp3lPF54NSq+tqQ8z5Idw7oDrpP6ScCf1hVN7agO5rupPlNrf6sti3DeB/wjrZdb2nBtpzuSHId3dHCWxnif1NV3Up3YvzNwHrg34BntOqN7Zv92/gDdBd1fLSqLh+y/fNWqnz4kUarfZq/h66L6aZpbs6MlOT9wK9W1Yrpbos0EY8sNBJJfq91sexId+ns9+iuhBGQ5GlJfjOdA+m6iT4/3e2SJmNYaFSW03WB/IDusP+48jB20M505y1+TNfl9pd0l3NKM5LdUJKkXh5ZSJJ6zckbCe6xxx61dOnS6W6GJM0qV1999Q+ratFEdXMyLJYuXcrKlSunuxmSNKskuWWyOruhJEm9DAtJUi/DQpLUy7CQJPUyLCRJvQwLSVIvw0KS1MuwkCT1MiwkSb3m5De4t9TSk788Leu9+fQXT8t6JamPRxaSpF6GhSSpl2EhSeplWEiSehkWkqRehoUkqZdhIUnqZVhIknoZFpKkXoaFJKmXYSFJ6mVYSJJ6GRaSpF6GhSSpl2EhSeplWEiSehkWkqRehoUkqZdhIUnqZVhIknqNNCySvDHJqiT/keT8JNsn2TfJVUlWJ/lMku3atI9t46tb/dKB5ZzSyr+f5MhRtlmS9GgjC4ski4HXA8uq6teBbYHjgPcDH6yq/YC7gePbLMcDd7fyD7bpSHJAm+/pwFHAR5NsO6p2S5IebdTdUAuAxyVZAOwA3A4cDlzU6s8FjmnDy9s4rf6IJGnlF1TVT6vqJmA1cOCI2y1JGjCysKiq24APALfShcS9wNXAPVW1oU22FljchhcDa9q8G9r0uw+WTzDPLyU5IcnKJCvXrVs39RskSfPYKLuhdqU7KtgX2BvYka4baSSq6syqWlZVyxYtWjSq1UjSvDTKbqjnAzdV1bqq+jlwMXAIsLB1SwEsAW5rw7cB+wC0+l2AHw2WTzCPJGkrGGVY3AoclGSHdu7hCOBa4HLgpW2aFcAX2vAlbZxW//WqqlZ+XLtaal9gf+BbI2y3JGmcBf2TbJ6quirJRcB3gA3ANcCZwJeBC5K8t5Wd3WY5G/hkktXAeroroKiqVUkupAuaDcCJVfXQqNotSXq0kYUFQFWdCpw6rvhGJriaqar+C3jZJMs5DThtyhsoSRqK3+CWJPUyLCRJvQwLSVIvw0KS1MuwkCT1MiwkSb0MC0lSL8NCktTLsJAk9TIsJEm9DAtJUi/DQpLUy7CQJPUyLCRJvQwLSVIvw0KS1MuwkCT1MiwkSb0MC0lSL8NCktTLsJAk9TIsJEm9DAtJUi/DQpLUy7CQJPUyLCRJvQwLSVIvw0KS1MuwkCT1MiwkSb0MC0lSL8NCktTLsJAk9TIsJEm9DAtJUi/DQpLUy7CQJPUaaVgkWZjkoiTXJ7kuycFJdktyaZIb2s9d27RJ8pEkq5P8e5JnDyxnRZv+hiQrRtlmSdKjjfrI4sPAV6rqacAzgOuAk4HLqmp/4LI2DvBCYP/2OgE4AyDJbsCpwHOBA4FTxwJGkrR1jCwskuwC/DZwNkBV/ayq7gGWA+e2yc4FjmnDy4HzqnMlsDDJXsCRwKVVtb6q7gYuBY4aVbslSY82yiOLfYF1wMeTXJPkrCQ7AntW1e1tmjuAPdvwYmDNwPxrW9lk5Y+Q5IQkK5OsXLdu3RRviiTNb6MMiwXAs4EzqupZwI95uMsJgKoqoKZiZVV1ZlUtq6plixYtmopFSpKaUYbFWmBtVV3Vxi+iC487W/cS7eddrf42YJ+B+Ze0ssnKJUlbycjCoqruANYkeWorOgK4FrgEGLuiaQXwhTZ8CfDKdlXUQcC9rbvqq8ALkuzaTmy/oJVJkraSBSNe/uuATyfZDrgReDVdQF2Y5HjgFuDYNu3fAy8CVgMPtmmpqvVJ3gN8u0337qpaP+J2S5IGjDQsqurfgGUTVB0xwbQFnDjJcs4BzpnSxkmShuY3uCVJvQwLSVIvw0KS1MuwkCT1MiwkSb0MC0lSL8NCktTLsJAk9TIsJEm9DAtJUq+hwiLJIcOUSZLmpmGPLP7fkGWSpDloozcSTHIw8DxgUZI3DVQ9Hth2lA2TJM0cfXed3Q7YqU2380D5fcBLR9UoSdLMstGwqKp/BP4xySeq6pat1CZJ0gwz7PMsHpvkTGDp4DxVdfgoGiVJmlmGDYvPAh8DzgIeGl1zJEkz0bBhsaGqzhhpSyRJM9awl85+Mclrk+yVZLex10hbJkmaMYY9sljRfr51oKyAJ01tcyRJM9FQYVFV+466IZKkmWuosEjyyonKq+q8qW2OJGkmGrYb6jkDw9sDRwDfAQwLSZoHhu2Get3geJKFwAWjaJAkaebZ3FuU/xjwPIYkzRPDnrP4It3VT9DdQPDXgAtH1ShJ0swy7DmLDwwMbwBuqaq1I2iPJGkGGqobqt1Q8Hq6O8/uCvxslI2SJM0swz4p71jgW8DLgGOBq5J4i3JJmieG7Yb6X8BzquougCSLgK8BF42qYZKkmWPYq6G2GQuK5kebMK8kaZYb9sjiK0m+Cpzfxl8O/P1omiRJmmn6nsG9H7BnVb01yR8Ah7aqfwU+PerGSZJmhr4jiw8BpwBU1cXAxQBJfqPV/d4I2yZJmiH6zjvsWVXfG1/YypaOpEWSpBmnLywWbqTucVPYDknSDNYXFiuTvGZ8YZI/Ba4eZgVJtk1yTZIvtfF9k1yVZHWSzyTZrpU/to2vbvVLB5ZxSiv/fpIjh946SdKU6DtncRLw+SSv4OFwWAZsB/z+kOt4A3Ad8Pg2/n7gg1V1QZKPAccDZ7Sfd1fVfkmOa9O9PMkBwHHA04G9ga8leUpVPTTk+iVJW2ijRxZVdWdVPQ94F3Bze72rqg6uqjv6Fp5kCfBi4Kw2HuBwHv4y37nAMW14eRun1R/Rpl8OXFBVP62qm4DVwIFDbp8kaQoM+zyLy4HLN2P5HwLeRndPKYDdgXuqakMbXwssbsOLgTVtfRuS3NumXwxcObDMwXkkSVvByL6FneRo4K6qGurcxhSs74QkK5OsXLdu3dZYpSTNG6O8ZcchwEuS3Ez3VL3DgQ8DC5OMHdEsAW5rw7cB+wC0+l3obivyy/IJ5vmlqjqzqpZV1bJFixZN/dZI0jw2srCoqlOqaklVLaU7Qf31qnoFXXfW2B1rVwBfaMOXtHFa/derqlr5ce1qqX2B/enugCtJ2kqGvTfUVHo7cEGS9wLXAGe38rOBTyZZDaynCxiqalWSC4Fr6R68dKJXQknS1rVVwqKqrgCuaMM3MsHVTFX1X3TPy5ho/tOA00bXQknSxnibcUlSL8NCktTLsJAk9TIsJEm9DAtJUi/DQpLUy7CQJPUyLCRJvQwLSVIvw0KS1MuwkCT1MiwkSb0MC0lSL8NCktTLsJAk9TIsJEm9DAtJUi/DQpLUy7CQJPUyLCRJvQwLSVIvw0KS1MuwkCT1MiwkSb0MC0lSL8NCktRrwXQ3QA9bevKXp2W9N5/+4mlZr6TZwyMLSVIvw0KS1MuwkCT1MiwkSb0MC0lSL8NCktTLsJAk9TIsJEm9/FKepu3LgOAXAqXZwiMLSVKvkYVFkn2SXJ7k2iSrkryhle+W5NIkN7Sfu7byJPlIktVJ/j3JsweWtaJNf0OSFaNqsyRpYqM8stgAvLmqDgAOAk5McgBwMnBZVe0PXNbGAV4I7N9eJwBnQBcuwKnAc4EDgVPHAkaStHWMLCyq6vaq+k4bvh+4DlgMLAfObZOdCxzThpcD51XnSmBhkr2AI4FLq2p9Vd0NXAocNap2S5Iebaucs0iyFHgWcBWwZ1Xd3qruAPZsw4uBNQOzrW1lk5WPX8cJSVYmWblu3bqp3QBJmudGHhZJdgI+B5xUVfcN1lVVATUV66mqM6tqWVUtW7Ro0VQsUpLUjDQskjyGLig+XVUXt+I7W/cS7eddrfw2YJ+B2Ze0ssnKJUlbySivhgpwNnBdVf3VQNUlwNgVTSuALwyUv7JdFXUQcG/rrvoq8IIku7YT2y9oZZKkrWSUX8o7BPgT4HtJ/q2V/TlwOnBhkuOBW4BjW93fAy8CVgMPAq8GqKr1Sd4DfLtN9+6qWj/CdkuSxhlZWFTVPwOZpPqICaYv4MRJlnUOcM7UtU6StCn8BrckqZdhIUnqZVhIknoZFpKkXoaFJKmXYSFJ6mVYSJJ6GRaSpF6GhSSpl2EhSeplWEiSehkWkqRehoUkqZdhIUnqZVhIknoZFpKkXoaFJKmXYSFJ6mVYSJJ6jewZ3NIwlp785WlZ782nv3ha1ivNVh5ZSJJ6GRaSpF6GhSSpl2EhSeplWEiSehkWkqRehoUkqZdhIUnqZVhIknoZFpKkXoaFJKmXYSFJ6mVYSJJ6GRaSpF6GhSSpl2EhSeplWEiSes2aJ+UlOQr4MLAtcFZVnT7NTdIsNl1P6AOf0qfZaVYcWSTZFvgb4IXAAcAfJTlgelslSfPHrAgL4EBgdVXdWFU/Ay4Alk9zmyRp3pgt3VCLgTUD42uB5w5OkOQE4IQ2+kCS72/iOvYAfrjZLZy55up2wSzdtry/d5JZuV1DmKvbBXNn2544WcVsCYteVXUmcObmzp9kZVUtm8ImzQhzdbtg7m6b2zX7zOVtGzNbuqFuA/YZGF/SyiRJW8FsCYtvA/sn2TfJdsBxwCXT3CZJmjdmRTdUVW1I8mfAV+kunT2nqlZN8Wo2uwtrhpur2wVzd9vcrtlnLm8bAKmq6W6DJGmGmy3dUJKkaWRYSJJ6zfuwSHJUku8nWZ3k5Oluz5ZIsk+Sy5Ncm2RVkje08t2SXJrkhvZz1+lu6+ZIsm2Sa5J8qY3vm+Sqtu8+0y5+mFWSLExyUZLrk1yX5OA5tL/e2H4P/yPJ+Um2n437LMk5Se5K8h8DZRPuo3Q+0rbv35M8e/paPrXmdVjMwduIbADeXFUHAAcBJ7btORm4rKr2By5r47PRG4DrBsbfD3ywqvYD7gaOn5ZWbZkPA1+pqqcBz6Dbvlm/v5IsBl4PLKuqX6e7MOU4Zuc++wRw1LiyyfbRC4H92+sE4Iyt1MaRm9dhwRy7jUhV3V5V32nD99P941lMt03ntsnOBY6ZlgZugSRLgBcDZ7XxAIcDF7VJZt12JdkF+G3gbICq+llV3cMc2F/NAuBxSRYAOwC3Mwv3WVV9A1g/rniyfbQcOK86VwILk+y1VRo6YvM9LCa6jcjiaWrLlEqyFHgWcBWwZ1Xd3qruAPacrnZtgQ8BbwN+0cZ3B+6pqg1tfDbuu32BdcDHW/faWUl2ZA7sr6q6DfgAcCtdSNwLXM3s32djJttHc/Z/ynwPizkpyU7A54CTquq+wbrqrpWeVddLJzkauKuqrp7utkyxBcCzgTOq6lnAjxnX5TQb9xdA68NfTheIewM78uiunDlhtu6jTTXfw2LO3UYkyWPoguLTVXVxK75z7FC4/bxrutq3mQ4BXpLkZrquwsPp+voXti4OmJ37bi2wtqquauMX0YXHbN9fAM8HbqqqdVX1c+Biuv042/fZmMn20Zz7nzJmvofFnLqNSOvHPxu4rqr+aqDqEmBFG14BfGFrt21LVNUpVbWkqpbS7aOvV9UrgMuBl7bJZuN23QGsSfLUVnQEcC2zfH81twIHJdmh/V6Obdus3mcDJttHlwCvbFdFHQTcO9BdNavN+29wJ3kRXX/42G1ETpveFm2+JIcC/wR8j4f79v+c7rzFhcATgFuAY6tq/Am7WSHJYcBbquroJE+iO9LYDbgG+OOq+uk0Nm+TJXkm3Un77YAbgVfTfYib9fsrybuAl9NdpXcN8Kd0/fezap8lOR84jO425HcCpwJ/xwT7qAXjX9N1uT0IvLqqVk5Ds6fcvA8LSVK/+d4NJUkagmEhSeplWEiSehkWkqRehoUkqZdhIW2CJA+MePknJdlha61PGpZhIc0sJ9HddE+aUWbFM7ilmSzJk+ludb+I7otYr6mq65N8ArgPWAb8KvC2qrooyTZ0X9w6nO6mcz8HzqG7h9LewOVJflhVv9OWfxpwNPATYHlV3bk1t08CjyykqXAm8Lqq+i3gLcBHB+r2Ag6l+2d/eiv7A2Ap3TNU/gQ4GKCqPgL8APidsaCguwHflVX1DOAbwGtGuiXSJDyykLZAu8Pv84DPdnd6AOCxA5P8XVX9Arg2ydhtrA8FPtvK70hy+UZW8TPgS234auB3p6zx0iYwLKQtsw3dMxqeOUn94H2PMsk0G/PzeviePA/h36ymid1Q0hZozwu5KcnL4JfPYH5Gz2zfBP4wyTbtaOOwgbr7gZ1H0lhpCxgW0qbZIcnagdebgFcAxyf5LrCK/kfzfo7uWRbXAp8CvkP3JDnozn98padrStrqvOusNA2S7FRVDyTZHfgWcEh7voU0I9n/KU2PLyVZSPcci/cYFJrpPLKQJPXynIUkqZdhIUnqZVhIknoZFpKkXoaFJKnX/wf1e+IMudzMagAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(list_len)\n",
    "plt.title(\"Histogram of Length of Sentences\")\n",
    "plt.xlabel(\"Length\")\n",
    "plt.ylabel(\"Count\")\n",
    "print(\"Final Length of sentences will be 30\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a2e79e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add pad token and unknown token in dictionary\n",
    "word2idx[\"PAD\"]=0\n",
    "label2idx[\"PAD\"]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50e72cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert a reverse dictionary of labels for mapping it back while creating the dev file output\n",
    "idx2label={label2idx[k] : k for k in label2idx}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f15e2dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pad sentences to the maximum length\n",
    "def padding_sentences(final_length, sentences, labels):\n",
    "    for i in range(len(sentences)):\n",
    "        lenn=len(sentences[i])\n",
    "        if lenn>=final_length:\n",
    "            sentences[i]=sentences[i][:final_length]\n",
    "            labels[i]=labels[i][:final_length]\n",
    "        else:\n",
    "            sentences[i]+=[0]*(final_length-lenn)\n",
    "            labels[i]+=[0]*(final_length-lenn)\n",
    "    return sentences, labels\n",
    "\n",
    "train_sentences, train_labels = padding_sentences(30, train_sentences, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7262621",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pad sentences to the maximum length\n",
    "max_train_sentences=30\n",
    "for i in range(len(train_sentences)):\n",
    "    lenn=len(train_sentences[i])\n",
    "    if lenn>=max_train_sentences:\n",
    "        train_sentences[i]=train_sentences[i][:max_train_sentences]\n",
    "        train_labels[i]=train_labels[i][:max_train_sentences]\n",
    "    else:\n",
    "        train_sentences[i]+=[0]*(max_train_sentences-lenn)\n",
    "        train_labels[i]+=[0]*(max_train_sentences-lenn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fcc2085d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_sentences(filename, word2idx, label2idx):\n",
    "#Same operation for dev data\n",
    "    val_sentences = []        \n",
    "    val_labels = []\n",
    "    temp_sentence=[]\n",
    "    temp_label=[]\n",
    "    unk_set=set()\n",
    "\n",
    "    with open(filename) as f:\n",
    "        for sentence in f.read().splitlines():\n",
    "            if sentence==\"\":\n",
    "                val_sentences.append(temp_sentence)\n",
    "                val_labels.append(temp_label)\n",
    "                temp_sentence=[]\n",
    "                temp_label=[]\n",
    "                continue\n",
    "            _,word,label=sentence.split()\n",
    "\n",
    "            if word not in word2idx:\n",
    "                unk_set.add(word)\n",
    "                temp_sentence.append(word2idx[\"UNK\"])\n",
    "            else:\n",
    "                temp_sentence.append(word2idx[word])\n",
    "\n",
    "            temp_label.append(label2idx[label])\n",
    "\n",
    "        val_sentences.append(temp_sentence)\n",
    "        val_labels.append(temp_label)\n",
    "    return val_sentences, val_labels, unk_set\n",
    "\n",
    "val_sentences, val_labels, dev_unk_set= gen_sentences(\"data/dev\", word2idx, label2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d196ffcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#padding of dev data\n",
    "padded_val_sentences= copy.deepcopy(val_sentences)\n",
    "padded_val_labels = copy.deepcopy(val_labels)\n",
    "padded_val_sentences, padded_val_labels = padding_sentences(30, padded_val_sentences, padded_val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73c257f",
   "metadata": {},
   "source": [
    "### Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2dbca98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myDataset(data.Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features=features\n",
    "        self.labels=labels\n",
    "        self.len = len(features)\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    def __getitem__(self, index):\n",
    "        row=self.features[index]\n",
    "        row_label=self.labels[index]\n",
    "        return torch.tensor(row),torch.tensor(row_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "547f0907",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set=myDataset(train_sentences, train_labels)\n",
    "training_generator = data.DataLoader(training_set, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7532ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_val_set=myDataset(padded_val_sentences, padded_val_labels)\n",
    "padded_val_generator = data.DataLoader(padded_val_set, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "645270bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, output_size, hidden_dim, embed_size, dropout_rate,  n_layers):\n",
    "        \n",
    "        super(Model_LSTM, self).__init__()\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        #Defining the layers\n",
    "        self.embedding=nn.Embedding(len(word2idx),embed_size, padding_idx=0)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        # LSTM Layer\n",
    "        self.lstm = nn.LSTM(input_size=embed_size, hidden_size=hidden_dim, batch_first=True, bidirectional=True)   \n",
    "        \n",
    "        #dense layer\n",
    "        self.fc = nn.Linear(2*hidden_dim, output_size)\n",
    "        self.elu=nn.ELU()\n",
    "        self.fc1 = nn.Linear(output_size, len(label2idx))\n",
    "        \n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        s=self.embedding(x)\n",
    "        s=self.dropout(s)\n",
    "        s, _ = self.lstm(s)\n",
    "        s=self.dropout(s)\n",
    "\n",
    "        s = self.fc(s)          \n",
    "\n",
    "        s=self.elu(s)\n",
    "\n",
    "        s=self.fc1(s)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        return s\n",
    "\n",
    "    \n",
    "    def init_weights(self):\n",
    "    # to initialize all parameters from normal distribution\n",
    "    # helps with converging during training\n",
    "        for name, param in self.named_parameters():\n",
    "            nn.init.normal_(param.data, mean=0, std=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b932a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model with hyperparameters\n",
    "model = Model_LSTM(output_size=128, hidden_dim=256, embed_size=100, dropout_rate=0.33,  n_layers=1)\n",
    "model.init_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7e15ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Loss, Optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a46acc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    #initialize every epoch \n",
    "#     epoch_loss = 0\n",
    "#     epoch_acc = 0\n",
    "    running_loss = 0.0\n",
    "#     running_corrects = 0\n",
    "    \n",
    "    #set the model in training phase\n",
    "    model.train()  \n",
    "    for x_input, y_label in iterator:\n",
    "        optimizer.zero_grad()   \n",
    "        \n",
    "        output= model(x_input)\n",
    "        output=output.view(-1, output.shape[-1])\n",
    "        y_label=y_label.long().view(-1)\n",
    "#         _, preds = torch.max(output, 1)\n",
    "\n",
    "        #compute the loss\n",
    "        loss = criterion(output, y_label)  \n",
    "#         loss = loss_fn(output, y_label)\n",
    "        \n",
    "        #compute the binary accuracy\n",
    "#         acc = binary_accuracy(output, batch.label)   \n",
    "        \n",
    "        #backpropage the loss and compute the gradients\n",
    "        loss.backward()     \n",
    "        \n",
    "        #clip gradient, to prevent from exploding\n",
    "#         nn.utils.clip_grad_norm_(model.parameters(), 0.7)\n",
    "        \n",
    "        #update the weights\n",
    "        optimizer.step()      \n",
    "\n",
    "        #loss and accuracy\n",
    "        running_loss += loss.item()\n",
    "#         running_corrects += torch.sum(preds == y_label).item()\n",
    "\n",
    "    epoch_loss = running_loss / len(iterator)\n",
    "#     epochs_acc = running_corrects / len(train_sentences)\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "    return epoch_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f3d6f680",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "\n",
    "    model.train(False)\n",
    "\n",
    "    running_loss = 0\n",
    "#     running_corrects = 0\n",
    "\n",
    "    for x_input, y_label in iterator:\n",
    "\n",
    "        output= model(x_input)\n",
    "        output=output.view(-1, output.shape[-1])\n",
    "        y_label=y_label.long().view(-1)\n",
    "#         _, preds = torch.max(output, 1)\n",
    "\n",
    "        loss = criterion(output, y_label)  \n",
    "\n",
    "        running_loss += loss.item()\n",
    "#         running_corrects += torch.sum(preds == y_label).item()\n",
    "\n",
    "    epoch_loss = running_loss / len(iterator)\n",
    "#     epochs_acc = running_corrects / len(val_sentences)\n",
    "\n",
    "    return epoch_loss\n",
    "# , epochs_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "02037366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\tTrain Loss: 0.636 | Val. Loss: 0.465\n",
      "Epoch: 2\tTrain Loss: 0.346 | Val. Loss: 0.276\n",
      "Epoch: 3\tTrain Loss: 0.228 | Val. Loss: 0.211\n",
      "Epoch: 4\tTrain Loss: 0.171 | Val. Loss: 0.167\n",
      "Epoch: 5\tTrain Loss: 0.138 | Val. Loss: 0.174\n",
      "Epoch: 6\tTrain Loss: 0.115 | Val. Loss: 0.175\n",
      "Epoch: 7\tTrain Loss: 0.099 | Val. Loss: 0.138\n",
      "Epoch: 8\tTrain Loss: 0.086 | Val. Loss: 0.152\n",
      "Epoch: 9\tTrain Loss: 0.076 | Val. Loss: 0.131\n",
      "Epoch: 10\tTrain Loss: 0.069 | Val. Loss: 0.143\n",
      "Epoch: 11\tTrain Loss: 0.064 | Val. Loss: 0.140\n",
      "Epoch: 12\tTrain Loss: 0.059 | Val. Loss: 0.136\n",
      "Epoch: 13\tTrain Loss: 0.055 | Val. Loss: 0.145\n",
      "Epoch: 14\tTrain Loss: 0.050 | Val. Loss: 0.145\n",
      "Epoch: 15\tTrain Loss: 0.047 | Val. Loss: 0.133\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 15\n",
    "# best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "     \n",
    "    #train the model\n",
    "#     train_loss, train_acc = train(model, training_generator, optimizer, criterion)\n",
    "    train_loss= train(model, training_generator, optimizer, criterion)\n",
    "    \n",
    "#     #evaluate the model\n",
    "#     valid_loss, valid_acc = evaluate(model, val_generator, criterion)\n",
    "    valid_loss = evaluate(model, padded_val_generator, criterion)\n",
    "    \n",
    "#     #save the best model\n",
    "#     if valid_loss < best_valid_loss:\n",
    "#         best_valid_loss = valid_loss\n",
    "#         torch.save(model.state_dict(), 'saved_weights.pt')\n",
    "    \n",
    "#     print(f'Epoch: {epoch+1}\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "#     print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "    print(f'Epoch: {epoch+1}\\tTrain Loss: {train_loss:.3f} | Val. Loss: {valid_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "066d4628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "filename = 'hw4_task1.pt'\n",
    "torch.save(model, filename)\n",
    " \n",
    "\n",
    "task1_model = torch.load(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d511209d",
   "metadata": {},
   "source": [
    "### Dev Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e8121933",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_set=myDataset(val_sentences, val_labels)\n",
    "val_generator = data.DataLoader(val_set, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "63631c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=[]\n",
    "for test_batch in val_generator:\n",
    "    pred=task1_model(test_batch[0]).squeeze().topk(1)[1].T[0].numpy()\n",
    "    y_pred.append(pred.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "269632ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_out(in_filename, out_filename, y_pred, idx2label):\n",
    "    open(out_filename, 'w').close()\n",
    "    f1 = open(out_filename, \"a\")\n",
    "\n",
    "    i=0\n",
    "    j=0\n",
    "    with open(in_filename) as f:\n",
    "        for sentence in f.read().splitlines():\n",
    "            if sentence==\"\":\n",
    "                i+=1\n",
    "                j=0\n",
    "                f1.write(\"\\n\")\n",
    "                continue\n",
    "    #         _,word,label=sentence.split()\n",
    "\n",
    "            f1.write(f'{sentence} {idx2label[y_pred[i][j]]}\\n')\n",
    "            j+=1 \n",
    "\n",
    "\n",
    "    f1.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "eaeac555",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_out(\"data/dev\", \"val_task1_final.txt\", y_pred, idx2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8298dab8",
   "metadata": {},
   "source": [
    "### Dev Data output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "176197ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_out_dev(in_filename, out_filename, y_pred, idx2label):\n",
    "    open(out_filename, 'w').close()\n",
    "    f1 = open(out_filename, \"a\")\n",
    "\n",
    "    i=0\n",
    "    j=0\n",
    "    with open(in_filename) as f:\n",
    "        for sentence in f.read().splitlines():\n",
    "            if sentence==\"\":\n",
    "                i+=1\n",
    "                j=0\n",
    "                f1.write(\"\\n\")\n",
    "                continue\n",
    "            idx,word,label=sentence.split()\n",
    "\n",
    "            f1.write(f'{idx} {word} {idx2label[y_pred[i][j]]}\\n')\n",
    "            j+=1 \n",
    "\n",
    "\n",
    "    f1.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080251c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_out_dev(\"data/dev\", \"dev_task1_final.txt\", y_pred, idx2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688f2b9a",
   "metadata": {},
   "source": [
    "### Test Data output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2ca1e766",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same operation for test data\n",
    "test_sentences = []        \n",
    "\n",
    "temp_sentence=[]\n",
    "\n",
    "test_unk_set=set()\n",
    "\n",
    "word_idx=1\n",
    "label_idx=1\n",
    "with open(\"data/test\") as f:\n",
    "    for sentence in f.read().splitlines():\n",
    "        if sentence==\"\":\n",
    "            test_sentences.append(temp_sentence)\n",
    "\n",
    "            temp_sentence=[]\n",
    " \n",
    "            continue\n",
    "        _,word=sentence.split()\n",
    "        \n",
    "        if word not in word2idx:\n",
    "            test_unk_set.add(word)\n",
    "            temp_sentence.append(word2idx[\"UNK\"])\n",
    "        else:\n",
    "            temp_sentence.append(word2idx[word])\n",
    "        \n",
    "\n",
    "            \n",
    "    test_sentences.append(temp_sentence)\n",
    "\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "2f8f7695",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "y_pred=[]\n",
    "for test_sentence in test_sentences:\n",
    "    pred=task1_model(torch.tensor(test_sentence).view(1,len(test_sentence))).squeeze().topk(1)[1].T[0].numpy()\n",
    "    y_pred.append(pred.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "43049c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_out(\"data/test\", \"test_task1_final.txt\", y_pred, idx2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f78f4c",
   "metadata": {},
   "source": [
    "## Task 2 - Using GloVe word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920732fa",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5ee2c2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx_new=copy.deepcopy(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "03b50edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dict = {}\n",
    "with open(\"glove.6B.100d.txt\", 'r', encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], \"float64\")\n",
    "        embeddings_dict[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6103400b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding 0 or 1 as the first element of the embedding deciding on whether the first letter is uppercase or lowercase.\n",
    "# This will help in providing case sensitivity as Glove embeddings are case insensitive\n",
    "glove_emb={}\n",
    "for word,idx in word2idx_new.items():\n",
    "    if word=='PAD' or word=='UNK':\n",
    "        pass\n",
    "    if word in embeddings_dict:\n",
    "        glove_emb[idx]=np.append(embeddings_dict[word],0.0)\n",
    "    elif word.lower() in embeddings_dict:\n",
    "        glove_emb[idx]=np.append(embeddings_dict[word.lower()],1.0)\n",
    "    else:\n",
    "        glove_emb[idx]=np.random.normal(scale=0.6, size=(101, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "565de6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_arr=np.array(list(glove_emb.values()))\n",
    "meann=np.mean(val_arr,axis=0)\n",
    "glove_emb[word2idx_new['PAD']]=np.zeros((101,))\n",
    "glove_emb[word2idx_new['UNK']]=meann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "54db7080",
   "metadata": {},
   "outputs": [],
   "source": [
    "l=len(glove_emb)\n",
    "\n",
    "for word in test_unk_set.union(dev_unk_set):\n",
    "    if word in embeddings_dict:\n",
    "        glove_emb[l]=np.append(embeddings_dict[word],0.0)\n",
    "    elif word.lower() in embeddings_dict:\n",
    "        glove_emb[l]=np.append(embeddings_dict[word.lower()],1.0)\n",
    "    else:\n",
    "        continue\n",
    "    word2idx_new[word]=l\n",
    "    l+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0fa42014",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_matrix=[]\n",
    "for i in range(len(glove_emb)):\n",
    "    emb_matrix.append(glove_emb[i])\n",
    "emb_matrix=np.array(emb_matrix, dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a995a9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences, train_labels, _= gen_sentences(\"data/train\", word2idx_new, label2idx)\n",
    "train_sentences, train_labels = padding_sentences(30, train_sentences, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "080848d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_sentences, val_labels, _= gen_sentences(\"data/dev\", word2idx_new, label2idx)\n",
    "padded_val_sentences= copy.deepcopy(val_sentences)\n",
    "padded_val_labels = copy.deepcopy(val_labels)\n",
    "padded_val_sentences, padded_val_labels = padding_sentences(30, padded_val_sentences, padded_val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845ce5b4",
   "metadata": {},
   "source": [
    "### Model - Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7e665c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set=myDataset(train_sentences, train_labels)\n",
    "training_generator = data.DataLoader(training_set, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "40faec53",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_val_set=myDataset(padded_val_sentences, padded_val_labels)\n",
    "padded_val_generator = data.DataLoader(padded_val_set, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "68dca614",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Model_Glove_LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, output_size, hidden_dim, embed_size, dropout_rate,  n_layers):\n",
    "        \n",
    "        super(Model_Glove_LSTM, self).__init__()\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "\n",
    "\n",
    "        self.embedding = nn.Embedding(len(word2idx_new),embed_size, padding_idx=0)\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(emb_matrix).float())\n",
    "\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        # LSTM Layer\n",
    "        self.lstm = nn.LSTM(input_size=embed_size, hidden_size=hidden_dim, batch_first=True, bidirectional=True)   \n",
    "        \n",
    "        #dense layer\n",
    "        self.fc = nn.Linear(2*hidden_dim, output_size)\n",
    "        self.elu=nn.ELU()\n",
    "        self.fc1 = nn.Linear(output_size, len(label2idx))\n",
    "        \n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        s=self.embedding(x)\n",
    "        s=self.dropout(s)\n",
    "        s, _ = self.lstm(s)\n",
    "        s=self.dropout(s)\n",
    "\n",
    "        s = self.fc(s)          \n",
    "\n",
    "        s=self.elu(s)\n",
    "\n",
    "        s=self.fc1(s)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        return s\n",
    "\n",
    "    \n",
    "    def init_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            nn.init.normal_(param.data, mean=0, std=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ac4793b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model with hyperparameters\n",
    "model = Model_Glove_LSTM(output_size=128, hidden_dim=256, embed_size=101, dropout_rate=0.33,  n_layers=1)\n",
    "# model.init_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a9c4f5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Loss, Optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "87e54ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\tTrain Loss: 0.390 | Val. Loss: 0.201\n",
      "Epoch: 2\tTrain Loss: 0.219 | Val. Loss: 0.131\n",
      "Epoch: 3\tTrain Loss: 0.179 | Val. Loss: 0.118\n",
      "Epoch: 4\tTrain Loss: 0.159 | Val. Loss: 0.099\n",
      "Epoch: 5\tTrain Loss: 0.142 | Val. Loss: 0.094\n",
      "Epoch: 6\tTrain Loss: 0.134 | Val. Loss: 0.086\n",
      "Epoch: 7\tTrain Loss: 0.125 | Val. Loss: 0.083\n",
      "Epoch: 8\tTrain Loss: 0.117 | Val. Loss: 0.076\n",
      "Epoch: 9\tTrain Loss: 0.109 | Val. Loss: 0.070\n",
      "Epoch: 10\tTrain Loss: 0.106 | Val. Loss: 0.067\n",
      "Epoch: 11\tTrain Loss: 0.098 | Val. Loss: 0.069\n",
      "Epoch: 12\tTrain Loss: 0.093 | Val. Loss: 0.065\n",
      "Epoch: 13\tTrain Loss: 0.090 | Val. Loss: 0.066\n",
      "Epoch: 14\tTrain Loss: 0.087 | Val. Loss: 0.060\n",
      "Epoch: 15\tTrain Loss: 0.083 | Val. Loss: 0.058\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 15\n",
    "# best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "     \n",
    "    train_loss= train(model, training_generator, optimizer, criterion)\n",
    "\n",
    "    valid_loss = evaluate(model, padded_val_generator, criterion)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1}\\tTrain Loss: {train_loss:.3f} | Val. Loss: {valid_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "128578ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "filename = 'hw4_task2.pt'\n",
    "torch.save(model, filename)\n",
    " \n",
    "task2_model = torch.load(filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7261296d",
   "metadata": {},
   "source": [
    "### Dev Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a068de75",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_set=myDataset(val_sentences, val_labels)\n",
    "val_generator = data.DataLoader(val_set, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1a115a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=[]\n",
    "for test_batch in val_generator:\n",
    "    pred=task2_model(test_batch[0]).squeeze().topk(1)[1].T[0].numpy()\n",
    "    y_pred.append(pred.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "49381e63",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "write_out(\"data/dev\", \"val_task2_final.txt\", y_pred, idx2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cac17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_out_dev(\"data/dev\", \"dev_task2_final.txt\", y_pred, idx2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5e0b15",
   "metadata": {},
   "source": [
    "### Test Data output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "6a35fe37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same operation for test data\n",
    "test_sentences = []        \n",
    "\n",
    "temp_sentence=[]\n",
    "\n",
    "test_unk_set=set()\n",
    "\n",
    "word_idx=1\n",
    "label_idx=1\n",
    "with open(\"data/test\") as f:\n",
    "    for sentence in f.read().splitlines():\n",
    "        if sentence==\"\":\n",
    "            test_sentences.append(temp_sentence)\n",
    "\n",
    "            temp_sentence=[]\n",
    " \n",
    "            continue\n",
    "        _,word=sentence.split()\n",
    "        \n",
    "        if word not in word2idx_new:\n",
    "            test_unk_set.add(word)\n",
    "            temp_sentence.append(word2idx_new[\"UNK\"])\n",
    "        else:\n",
    "            temp_sentence.append(word2idx_new[word])\n",
    "        \n",
    "\n",
    "            \n",
    "    test_sentences.append(temp_sentence)\n",
    "\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "f9ba1462",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "y_pred=[]\n",
    "for test_sentence in test_sentences:\n",
    "    pred=task2_model(torch.tensor(test_sentence).view(1,len(test_sentence))).squeeze().topk(1)[1].T[0].numpy()\n",
    "    y_pred.append(pred.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "90aae341",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_out(\"data/test\", \"test_task2_final.txt\", y_pred, idx2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45880a38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
