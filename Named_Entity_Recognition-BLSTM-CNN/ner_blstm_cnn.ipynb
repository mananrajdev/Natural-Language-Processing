{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from collections import defaultdict\n",
    "import torch.optim as optim\n",
    "import warnings\n",
    "import sys\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_train_sentences = []        \n",
    "train_labels = []\n",
    "temp_sentence=[]\n",
    "temp_labels=[]\n",
    "tag_set=set()\n",
    "\n",
    "with open(\"data/train\") as f:\n",
    "    for sentence in f.read().splitlines():\n",
    "        if sentence==\"\":\n",
    "            word_train_sentences.append(temp_sentence)\n",
    "            train_labels.append(temp_labels)\n",
    "            temp_sentence=[]\n",
    "            temp_labels=[]\n",
    "            continue\n",
    "        _,word,label=sentence.split()\n",
    "        temp_sentence.append(word)\n",
    "        temp_labels.append(label)\n",
    "        tag_set.add(label)    \n",
    "    word_train_sentences.append(temp_sentence)\n",
    "    train_labels.append(temp_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dev_sentences = []        \n",
    "dev_labels = []\n",
    "temp_sentence=[]\n",
    "temp_labels=[]\n",
    "\n",
    "\n",
    "with open(\"data/dev\") as f:\n",
    "    for sentence in f.read().splitlines():\n",
    "        if sentence==\"\":\n",
    "            word_dev_sentences.append(temp_sentence)\n",
    "            dev_labels.append(temp_labels)\n",
    "            temp_sentence=[]\n",
    "            temp_labels=[]\n",
    "            continue\n",
    "        _,word,label=sentence.split()\n",
    "        temp_sentence.append(word)\n",
    "        temp_labels.append(label)\n",
    "            \n",
    "    word_dev_sentences.append(temp_sentence)\n",
    "    dev_labels.append(temp_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_test_sentences = []        \n",
    "temp_sentence=[]\n",
    "\n",
    "\n",
    "with open(\"data/test\") as f:\n",
    "    for sentence in f.read().splitlines():\n",
    "        if sentence==\"\":\n",
    "            word_test_sentences.append(temp_sentence)\n",
    "            temp_sentence=[]\n",
    "            continue\n",
    "        _,word=sentence.split()\n",
    "        temp_sentence.append(word)\n",
    "            \n",
    "    word_test_sentences.append(temp_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_token = 'PAD'\n",
    "unk_token = 'UNK'\n",
    "pad_id = 0\n",
    "unk_id = 1\n",
    "\n",
    "label2idx = {'PAD': 0}\n",
    "word2idx = {'PAD': 0, 'UNK': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dictionary for mapping word to integer and label to integer for creating their embeddings\n",
    "df_train=pd.read_csv(\"../data/train\", sep=\"\\s\", names=[\"idx\",\"word\",\"tag\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count=df_train['word'].value_counts()\n",
    "word_count=word_count[word_count>1]\n",
    "word_set=list(set(word_count.index))\n",
    "tag_set=list(tag_set)\n",
    "for i in range(len(word_set)):\n",
    "    word2idx[word_set[i]]=i+2\n",
    "\n",
    "for i in range(len(tag_set)):\n",
    "    label2idx[tag_set[i]]=i+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2label={label2idx[k] : k for k in label2idx}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pad sentences to the maximum length\n",
    "def padding_sentences(final_length, sentences, labels):\n",
    "    train_sentences=[]\n",
    "    train_labels=[]\n",
    "    for i in range(len(sentences)):\n",
    "        lenn=len(sentences[i])\n",
    "        temp_words=[]\n",
    "        temp_labels=[]\n",
    "        for j in range(final_length):\n",
    "            if j<lenn:\n",
    "                word = word2idx[sentences[i][j]] if sentences[i][j] in word2idx else 1\n",
    "                label = label2idx[labels[i][j]] if labels[i][j] in label2idx else 1\n",
    "                temp_words.append(word)\n",
    "                temp_labels.append(label)\n",
    "            else:\n",
    "                temp_words.append(0)\n",
    "                temp_labels.append(0)\n",
    "        train_sentences.append(temp_words)\n",
    "        train_labels.append(temp_labels)\n",
    "\n",
    "    return torch.tensor(train_sentences, dtype=torch.long), torch.tensor(train_labels, dtype=torch.long)\n",
    "\n",
    "train_sentences, train_labels = padding_sentences(28, word_train_sentences, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_set=set()\n",
    "for seq in word_train_sentences:\n",
    "    for word in seq:\n",
    "        for c in word:\n",
    "            char_set.add(c)\n",
    "        \n",
    "char2idx={'PAD': 0}      \n",
    "char_set=list(char_set)\n",
    "for i in range(len(char_set)):\n",
    "    char2idx[char_set[i]]=i+1\n",
    "\n",
    "idx2char={label2idx[k] : k for k in label2idx}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word={word2idx[k] : k for k in word2idx}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_char_len=19\n",
    "def gen_char_sentences(dataset):\n",
    "    char_sentences=[]\n",
    "    for sentence in dataset:\n",
    "        word_temp=[]\n",
    "        for idx in sentence:\n",
    "            word=idx2word[int(idx.numpy())]\n",
    "            l=len(word)\n",
    "            temp=[]\n",
    "            if l<max_char_len:\n",
    "                s=(max_char_len-l)//2\n",
    "                e= max_char_len-l-s\n",
    "                temp.extend([0]*s)\n",
    "                for c in word:\n",
    "                    temp.append(char2idx[c])\n",
    "                temp.extend([0]*e)\n",
    "            else:\n",
    "                for i in range(max_char_len):\n",
    "                    temp.append(char2idx[word[i]])\n",
    "            word_temp.append(temp)\n",
    "        char_sentences.append(word_temp)\n",
    "    return torch.tensor(char_sentences, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_chars=gen_char_sentences(train_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_glove = 'glove.6B.100d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dict = {}\n",
    "with open(filepath_glove, 'r', encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        embeddings_dict[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in word_dev_sentences:\n",
    "    for word in sentence:\n",
    "        if word not in word2idx:\n",
    "            word2idx[word]=len(word2idx)\n",
    "            \n",
    "for sentence in word_test_sentences:\n",
    "    for word in sentence:\n",
    "        if word not in word2idx:\n",
    "            word2idx[word]=len(word2idx)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_matrix=np.random.normal(0, 0.1, (len(word2idx), 100))\n",
    "\n",
    "for word,index in word2idx.items():\n",
    "    word = word if word in embeddings_dict else word.lower()\n",
    "   \n",
    "    if word in embeddings_dict:\n",
    "        emb_matrix[index]=torch.as_tensor(embeddings_dict[word])    \n",
    "    \n",
    "emb_matrix=torch.tensor(emb_matrix, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_Glove_LSTM_CNN(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_dim,\n",
    "                 embedding_dim,\n",
    "                 char_emb_dim,  \n",
    "                 char_input_dim,  \n",
    "                 char_cnn_filter_num,  \n",
    "                 char_cnn_kernel_size,  \n",
    "                 hidden_dim,\n",
    "                 linear_output_dim,\n",
    "                 lstm_layers,\n",
    "                 emb_dropout,\n",
    "                 cnn_dropout,  \n",
    "                 fc_dropout,\n",
    "                 word_pad_idx,\n",
    "                 char_pad_idx):  \n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=input_dim,\n",
    "            embedding_dim=embedding_dim,\n",
    "            padding_idx=word_pad_idx\n",
    "        )\n",
    "        self.emb_dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "\n",
    "        self.char_emb_dim = char_emb_dim\n",
    "        self.char_emb = nn.Embedding(\n",
    "            num_embeddings=char_input_dim,\n",
    "            embedding_dim=char_emb_dim,\n",
    "            padding_idx=char_pad_idx\n",
    "        )\n",
    "        self.char_cnn = nn.Conv1d(\n",
    "            in_channels=char_emb_dim,\n",
    "            out_channels=char_emb_dim * char_cnn_filter_num,\n",
    "            kernel_size=char_cnn_kernel_size,\n",
    "            groups=char_emb_dim  \n",
    "        )\n",
    "        self.cnn_dropout = nn.Dropout(cnn_dropout)\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim + (char_emb_dim * char_cnn_filter_num),\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=lstm_layers,\n",
    "            bidirectional=True, batch_first=True)\n",
    "      \n",
    "        self.fc_dropout = nn.Dropout(fc_dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, linear_output_dim)  \n",
    "        \n",
    "        \n",
    "      \n",
    "        self.elu = nn.ELU(alpha=1.0, inplace=False)\n",
    "        \n",
    "      \n",
    "        self.linear_classifier = nn.Linear(linear_output_dim, len(label2idx))\n",
    "        \n",
    "        \n",
    "        for name, param in self.named_parameters():\n",
    "            nn.init.normal_(param.data, mean=0, std=0.1)\n",
    "\n",
    "    def forward(self, words, chars):\n",
    "      \n",
    "        embedding_out = self.emb_dropout(self.embedding(words))\n",
    "     \n",
    "        char_emb_out = self.emb_dropout(self.char_emb(chars))\n",
    "        batch_size, sent_len, word_len, char_emb_dim = char_emb_out.shape\n",
    "        char_cnn_max_out = torch.zeros(batch_size, sent_len, self.char_cnn.out_channels)\n",
    "     \n",
    "        for sent_i in range(sent_len):\n",
    "            \n",
    "            sent_char_emb = char_emb_out[:, sent_i, :, :]  \n",
    "           \n",
    "            sent_char_emb_p = sent_char_emb.permute(0, 2, 1)  \n",
    "            \n",
    "            char_cnn_sent_out = self.char_cnn(sent_char_emb_p)\n",
    "            char_cnn_max_out[:, sent_i, :], _ = torch.max(char_cnn_sent_out, dim=2) \n",
    "        char_cnn = self.cnn_dropout(char_cnn_max_out)\n",
    "\n",
    "    \n",
    "        word_features = torch.cat((embedding_out, char_cnn), dim=2)\n",
    "\n",
    "        lstm_out, _ = self.lstm(word_features)\n",
    " \n",
    "        s = self.fc(self.fc_dropout(lstm_out))\n",
    "        \n",
    "        s = self.elu(s)\n",
    "        s = self.linear_classifier(s)\n",
    "        return s\n",
    "\n",
    "    def init_embeddings(self, char_pad_idx, word_pad_idx, pretrained=None, freeze=True):\n",
    "      \n",
    "        self.embedding.weight.data[word_pad_idx] = torch.zeros(self.embedding_dim)\n",
    "        self.char_emb.weight.data[char_pad_idx] = torch.zeros(self.char_emb_dim)\n",
    "        if pretrained is not None:\n",
    "            self.embedding = nn.Embedding.from_pretrained(\n",
    "                embeddings=torch.as_tensor(pretrained),\n",
    "                padding_idx=word_pad_idx,\n",
    "                freeze=freeze\n",
    "            )\n",
    "\n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 1,110,488 trainable parameters.\n",
      "Model_Glove_LSTM_CNN(\n",
      "  (embedding): Embedding(20610, 100, padding_idx=0)\n",
      "  (emb_dropout): Dropout(p=0.33, inplace=False)\n",
      "  (char_emb): Embedding(85, 30, padding_idx=0)\n",
      "  (char_cnn): Conv1d(30, 150, kernel_size=(3,), stride=(1,), groups=30)\n",
      "  (cnn_dropout): Dropout(p=0.25, inplace=False)\n",
      "  (lstm): LSTM(250, 256, batch_first=True, bidirectional=True)\n",
      "  (fc_dropout): Dropout(p=0.33, inplace=False)\n",
      "  (fc): Linear(in_features=512, out_features=128, bias=True)\n",
      "  (elu): ELU(alpha=1.0)\n",
      "  (linear_classifier): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Model_Glove_LSTM_CNN(\n",
    "    input_dim=len(word2idx),\n",
    "    embedding_dim=100,\n",
    "    char_emb_dim=30,\n",
    "    char_input_dim=len(char2idx),\n",
    "    char_cnn_filter_num=5,\n",
    "    char_cnn_kernel_size=3,\n",
    "    hidden_dim=256,\n",
    "    linear_output_dim=128,\n",
    "    lstm_layers=1,\n",
    "    emb_dropout=0.33,\n",
    "    cnn_dropout=0.25,\n",
    "    fc_dropout=0.33,\n",
    "    word_pad_idx=0,\n",
    "    char_pad_idx=0\n",
    ")\n",
    "model.init_embeddings(\n",
    "    char_pad_idx=0,\n",
    "    word_pad_idx=0,\n",
    "    pretrained=emb_matrix,\n",
    "    freeze=True\n",
    ")\n",
    "print(f\"The model has {model.count_parameters():,} trainable parameters.\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "## changing this to suit my input\n",
    "\n",
    "class Task3(object):\n",
    "\n",
    "    def __init__(self, model, data, optimizer_cls, loss_fn_cls, LR, Momentum):\n",
    "        self.model = model\n",
    "        self.data = data\n",
    "        self.optimizer = optimizer_cls(model.parameters(), lr=LR, momentum = Momentum)\n",
    "       \n",
    "        self.loss_fn = loss_fn_cls(ignore_index=self.data.tag_pad_idx)\n",
    "       \n",
    "        \n",
    "   \n",
    "    def accuracy(self, preds, y):\n",
    "        max_preds = preds.argmax(dim=1, keepdim=True)  # get the index of the max probability\n",
    "        non_pad_elements = (y != self.data.tag_pad_idx).nonzero()  # prepare masking for paddings\n",
    "        correct = max_preds[non_pad_elements].squeeze(1).eq(y[non_pad_elements])\n",
    "        return correct.sum() / torch.FloatTensor([y[non_pad_elements].shape[0]])\n",
    "    \n",
    "    \n",
    "    def epoch(self):\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        self.model.train()\n",
    "        for words,true_tags,chars in self.data.train_iter:\n",
    "              \n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            pred_tags = self.model(words, chars)  # MODIFIED\n",
    "       \n",
    "            pred_tags = pred_tags.view(-1, pred_tags.shape[-1])\n",
    "            \n",
    "        \n",
    "            true_tags = true_tags.view(-1)\n",
    "            batch_loss = self.loss_fn(pred_tags, true_tags)\n",
    "            batch_acc = self.accuracy(pred_tags, true_tags)\n",
    "            batch_loss.backward()\n",
    "            self.optimizer.step()\n",
    "            epoch_loss += batch_loss.item()\n",
    "            epoch_acc += batch_acc.item()\n",
    "        return epoch_loss / len(self.data.train_iter), epoch_acc / len(self.data.train_iter)\n",
    "\n",
    "  \n",
    "\n",
    "    def evaluate(self, iterator):\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "        \n",
    "            for batch in iterator:\n",
    "                \n",
    "                words = batch[0]\n",
    "                chars = batch[2] \n",
    "                true_tags = batch[1]\n",
    "                pred_tags = self.model(words, chars)  # MODIFIED\n",
    "                pred_tags = pred_tags.view(-1, pred_tags.shape[-1])\n",
    "                true_tags = true_tags.view(-1)\n",
    "                batch_loss = self.loss_fn(pred_tags, true_tags)\n",
    "                batch_acc = self.accuracy(pred_tags, true_tags)\n",
    "                epoch_loss += batch_loss.item()\n",
    "                epoch_acc += batch_acc.item()\n",
    "                \n",
    "        return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    " \n",
    "    def train(self, n_epochs):\n",
    "        for epoch in range(n_epochs):\n",
    "           \n",
    "            \n",
    "            train_loss, train_acc = self.epoch()\n",
    "            \n",
    "          \n",
    "            print(f\"Epoch: {epoch + 1:02} \")\n",
    "            print(f\"\\tTrn Loss: {train_loss:.3f} \")\n",
    "            \n",
    "            val_loss, val_acc = self.evaluate(self.data.val_iter)\n",
    "            print(f\"\\tVal Loss: {val_loss:.3f} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting input into train and validation \n",
    "\n",
    "num_workers=0\n",
    "batch_size = 16\n",
    "\n",
    "indices = list(range(len(word_train_sentences)))\n",
    "split = int(len(word_train_sentences)*0.1)\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# define samplers for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "input_dataset = torch.utils.data.TensorDataset(train_sentences, label_encoded,train_data_chars)\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader = torch.utils.data.DataLoader(input_dataset, batch_size=batch_size,\n",
    "                                            sampler=train_sampler, num_workers=num_workers)\n",
    "valid_loader = torch.utils.data.DataLoader(input_dataset, batch_size=batch_size, \n",
    "                                            sampler=valid_sampler, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class train_valid(object):\n",
    "    def __init__(self,train_loader,valid_loader):\n",
    "        self.train_iter = train_loader\n",
    "        self.val_iter = valid_loader\n",
    "        self.tag_pad_idx = pad_id\n",
    "        self.unk_id = unk_id\n",
    "        self.word2idx_dev_test = word2idx\n",
    "        self.tag2idx = label2idx\n",
    "        \n",
    "data1=train_valid(train_loader,valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 \n",
      "\tTrn Loss: 0.059 \n",
      "\tVal Loss: 0.079 \n",
      "Epoch: 02 \n",
      "\tTrn Loss: 0.055 \n",
      "\tVal Loss: 0.077 \n",
      "Epoch: 03 \n",
      "\tTrn Loss: 0.054 \n",
      "\tVal Loss: 0.074 \n",
      "Epoch: 04 \n",
      "\tTrn Loss: 0.051 \n",
      "\tVal Loss: 0.084 \n",
      "Epoch: 05 \n",
      "\tTrn Loss: 0.047 \n",
      "\tVal Loss: 0.074 \n",
      "Epoch: 06 \n",
      "\tTrn Loss: 0.047 \n",
      "\tVal Loss: 0.081 \n",
      "Epoch: 07 \n",
      "\tTrn Loss: 0.045 \n",
      "\tVal Loss: 0.078 \n",
      "Epoch: 08 \n",
      "\tTrn Loss: 0.043 \n",
      "\tVal Loss: 0.078 \n",
      "Epoch: 09 \n",
      "\tTrn Loss: 0.041 \n",
      "\tVal Loss: 0.072 \n",
      "Epoch: 10 \n",
      "\tTrn Loss: 0.040 \n",
      "\tVal Loss: 0.084 \n",
      "Epoch: 11 \n",
      "\tTrn Loss: 0.039 \n",
      "\tVal Loss: 0.080 \n",
      "Epoch: 12 \n",
      "\tTrn Loss: 0.037 \n",
      "\tVal Loss: 0.079 \n",
      "Epoch: 13 \n",
      "\tTrn Loss: 0.035 \n",
      "\tVal Loss: 0.082 \n",
      "Epoch: 14 \n",
      "\tTrn Loss: 0.034 \n",
      "\tVal Loss: 0.082 \n",
      "Epoch: 15 \n",
      "\tTrn Loss: 0.033 \n",
      "\tVal Loss: 0.072 \n"
     ]
    }
   ],
   "source": [
    "t3 = Task3(model=model, data=data1, optimizer_cls=optim.SGD, loss_fn_cls=nn.CrossEntropyLoss, LR=0.1, Momentum=0.9)\n",
    "t3.train(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "filename = 'blstm3.pt'\n",
    "torch.save(model, filename)\n",
    " \n",
    "\n",
    "task3_model = torch.load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(sentence, true_tags=None):\n",
    "    model.eval()\n",
    "    word_list=[]\n",
    "    encoded_word_list=[]\n",
    "    for word in sentence:\n",
    "        word_list.append(word)\n",
    "        encoded_word_list.append(word2idx[word] if word in word2idx else 1)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    word_temp=[]\n",
    "    for word in sentence:\n",
    "        l=len(word)\n",
    "        temp=[]\n",
    "        if l<max_char_len:\n",
    "            s=(max_char_len-l)//2\n",
    "            e= max_char_len-l-s\n",
    "            temp.extend([0]*s)\n",
    "            for c in word:\n",
    "                temp.append(char2idx[c] if c in char2idx else 0)\n",
    "            temp.extend([0]*e)\n",
    "        else:\n",
    "            for i in range(max_char_len):\n",
    "                temp.append(char2idx[word[i]] if word[i] in char2idx else 0)\n",
    "        word_temp.append(temp)\n",
    "    \n",
    "    \n",
    " \n",
    "    pred = model(torch.as_tensor(encoded_word_list).unsqueeze(0), torch.as_tensor(word_temp).unsqueeze(0)).argmax(-1)\n",
    "\n",
    "\n",
    "    return word_list, pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dev Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_filename=\"dev3.out\"\n",
    "open(out_filename, 'w').close()\n",
    "f1 = open(out_filename, \"a\")\n",
    "for i in range(len(word_dev_sentences)):\n",
    "    word_list, pred = infer(word_dev_sentences[i], dev_labels[i])\n",
    "    for j in range(len(word_list)):\n",
    "        f1.write(f'{j+1} {word_list[j]} {dev_labels[i][j]} {idx2label[int(pred[0][j].numpy())]}\\n')\n",
    "    f1.write(\"\\n\")\n",
    "f1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_filename=\"test3.out\"\n",
    "open(out_filename, 'w').close()\n",
    "f1 = open(out_filename, \"a\")\n",
    "for i in range(len(word_test_sentences)):\n",
    "    word_list, pred = infer(word_test_sentences[i])\n",
    "    for j in range(len(word_list)):\n",
    "        f1.write(f'{j+1} {word_list[j]} {idx2label[int(pred[0][j].numpy())]}\\n')\n",
    "    f1.write(\"\\n\")\n",
    "f1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
